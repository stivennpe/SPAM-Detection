{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b80643f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import math\n",
    "\n",
    "# helps in text preprocessing\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# helps in model building\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import pickle\n",
    "\n",
    "#read Religion topic\n",
    "religion_path = \"C:/Users/steven/Documents/spam/Topic_00.en\"\n",
    "#save each sentence into np.array\n",
    "with open(religion_path, encoding=\"utf8\") as f:\n",
    "    relig_content = [line.rstrip() for line in f]\n",
    "rel_arr = np.array(relig_content)\n",
    "#add for each sentence the same class (e.g.: 0)\n",
    "rel_arr_label = np.full(len(rel_arr), 0)\n",
    "\n",
    "import random\n",
    "import math\n",
    "random.seed(42)\n",
    "#read all other topics\n",
    "other_topic_numb = 9\n",
    "max_topic_numb = 9\n",
    "general_arr = []\n",
    "count = 0\n",
    "segments_pro_topic = math.ceil(len(rel_arr) / other_topic_numb)\n",
    "while count <= max_topic_numb:\n",
    "    if(count != 0):#to skip undesired topics\n",
    "        general_path = f\"C:/Users/steven/Documents/spam/Topic_{count:02d}.en\"\n",
    "        #save each sentence into np.array\n",
    "        #print(segments_pro_topic)\n",
    "        #save some sentences into np.array (the same amount as Religion topic ones)\n",
    "        with open(general_path, encoding=\"utf8\") as f:\n",
    "            general_arr = general_arr + (random.sample([line.rstrip() for line in f], segments_pro_topic))\n",
    "    count = count + 1\n",
    "\n",
    "#print(len(general_arr))\n",
    "random.shuffle(general_arr)\n",
    "#add for each sentence the same class (Different from Religion! e.g.: 1)\n",
    "general_arr_label = np.full(len(general_arr), 1)\n",
    "\n",
    "conc_arr = np.concatenate((rel_arr,general_arr))\n",
    "conc_arr_label = np.concatenate((rel_arr_label,general_arr_label))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(conc_arr, conc_arr_label, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab609855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102234\n",
      "Maximum value: 200\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "t = Tokenizer()\n",
    "t.fit_on_texts(x_train)\n",
    "\n",
    "print(len(t.word_index))\n",
    "word_index = t.word_index\n",
    "encoded_train = t.texts_to_sequences(x_train)\n",
    "encoded_test = t.texts_to_sequences(x_test)\n",
    "\n",
    "length=[]\n",
    "for conc in conc_arr:\n",
    "    #print(len(conc))\n",
    "    split = conc.split(\" \")\n",
    "    #print(len(test1))\n",
    "    length.append(len(split))\n",
    "max_value = max(length)\n",
    "print('Maximum value:', max_value)\n",
    "\n",
    "max_length = max_value\n",
    "\n",
    "padded_train = pad_sequences(encoded_train, maxlen=max_length, padding='post')\n",
    "padded_test = pad_sequences(encoded_test, maxlen=max_length, padding='post')\n",
    "#print(len(padded_train))\n",
    "\n",
    "#word_index = dict(zip((o for o in x_train), range(len(x_train))))\n",
    "#print(word_index)\n",
    "#print(len(encoded_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "709b793a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "#make a dict mapping words (strings) to their NumPy vector representation:\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "path_to_glove_file = \"C:/Users/Steven/Downloads/glove.6B.100d.txt/glove.6B.100d.txt\"\n",
    "\n",
    "embeddings_index = {}\n",
    "with open(path_to_glove_file, encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print(\"Found %s word vectors.\" % len(embeddings_index)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45a9bf95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 62670 words (39564 misses)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "NP matrix for the \"embedding\" layer in Keras. To map index of pretrained vectos with the word of index i in the vectorizer vocab\n",
    "\"\"\"\n",
    "\n",
    "num_tokens = len(t.word_index) + 2\n",
    "embedding_dim = 100\n",
    "hits = 0\n",
    "misses = 0\n",
    "\n",
    "# Prepare embedding matrix\n",
    "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # Words not found in embedding index will be all-zeros.\n",
    "        # This includes the representation for \"padding\" and \"OOV\"\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        hits += 1\n",
    "    else:\n",
    "        misses += 1\n",
    "print(\"Converted %d words (%d misses)\" % (hits, misses))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "83243c28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.34648  ,  0.2096   ,  0.070741 , -0.97231  , -0.052933 ,\n",
       "        0.65437  , -0.86157  , -0.69758  ,  0.25109  , -0.32881  ,\n",
       "        0.81759  , -0.34523  ,  0.53054  ,  0.068022 ,  0.20367  ,\n",
       "       -0.96712  ,  0.6745   , -0.050067 , -1.6481   ,  0.86177  ,\n",
       "        0.11933  , -0.96415  , -0.84824  ,  0.029813 ,  0.29883  ,\n",
       "        1.2351   , -0.62322  , -0.0096935,  0.57381  ,  0.26418  ,\n",
       "        0.35808  ,  0.040504 ,  0.13176  ,  0.19399  ,  0.11848  ,\n",
       "        0.31298  , -0.22346  ,  0.29079  ,  0.35653  , -0.22875  ,\n",
       "       -0.65771  ,  1.1989   ,  0.26678  , -0.47767  ,  0.10477  ,\n",
       "       -0.58318  ,  0.1232   ,  0.14058  , -0.12584  , -0.50352  ,\n",
       "       -0.58613  ,  0.82644  ,  1.1736   ,  0.78827  , -1.0276   ,\n",
       "        0.3125   ,  0.81627  , -0.13461  ,  0.02046  , -0.50721  ,\n",
       "        0.31498  ,  0.077944 , -0.66653  , -0.43601  ,  0.040687 ,\n",
       "        0.69847  ,  0.36838  , -0.30218  , -0.37972  , -0.37998  ,\n",
       "        0.81017  ,  0.26011  ,  1.0392   , -1.5402   ,  0.56638  ,\n",
       "        0.70069  , -0.25647  , -0.71319  , -0.64077  ,  0.64458  ,\n",
       "        0.20351  , -0.80554  , -1.0013   , -0.33917  , -0.70438  ,\n",
       "        0.38687  , -0.48743  , -0.92918  ,  0.72704  ,  0.15937  ,\n",
       "        0.2959   ,  0.067437 ,  0.047466 ,  0.26232  , -0.1498   ,\n",
       "       -0.68157  ,  0.40106  , -0.50263  , -0.45964  ,  0.30936  ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_index.get(\"allah\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "25885bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "Load the pretrained Glove embeddings to the embedding layer in keras. Set Trainable False.\n",
    "\"\"\"\n",
    "import keras\n",
    "from tensorflow.keras.layers import Embedding\n",
    "\n",
    "embedding_layer = Embedding(\n",
    "    num_tokens,\n",
    "    embedding_dim,\n",
    "    embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
    "    trainable=False,\n",
    "    input_length=max_length\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e97e155b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build a model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "n_lstm = 20\n",
    "drop_lstm =0.2\n",
    "embed_size = 100 # how big is each word vector\n",
    "maxlen = 2000 # max number of words in a question to use\n",
    "max_features = 44971\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5360fb5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 200, 100)          10223600  \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 200, 40)           19360     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 200, 128)          5248      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 200, 1)            129       \n",
      "=================================================================\n",
      "Total params: 10,248,337\n",
      "Trainable params: 24,737\n",
      "Non-trainable params: 10,223,600\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
    "model = Sequential([\n",
    "    embedding_layer,\n",
    "    Bidirectional(LSTM(n_lstm, dropout=drop_lstm, input_length=max_length,  return_sequences=True)), \n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "748fa345",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "early_stop = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "863e4562",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "def c_report(y_true, y_pred):\n",
    "   print(\"Classification Report\")\n",
    "   print(classification_report(y_true, y_pred))\n",
    "   acc_sc = accuracy_score(y_true, y_pred)\n",
    "   print(\"Accuracy : \"+ str(acc_sc))\n",
    "   return acc_sc\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred):\n",
    "   mtx = confusion_matrix(y_true, y_pred)\n",
    "   sns.heatmap(mtx, annot=True, fmt='d', linewidths=.5, \n",
    "               cmap=\"Blues\", cbar=False)\n",
    "   plt.ylabel('True label')\n",
    "   plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "db522693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1833/1833 [==============================] - 692s 375ms/step - loss: 0.2859 - accuracy: 0.8807 - val_loss: 0.2203 - val_accuracy: 0.9123\n",
      "Epoch 2/5\n",
      "1833/1833 [==============================] - 732s 400ms/step - loss: 0.2250 - accuracy: 0.9068 - val_loss: 0.2299 - val_accuracy: 0.9014\n",
      "Epoch 3/5\n",
      "1833/1833 [==============================] - 750s 409ms/step - loss: 0.2108 - accuracy: 0.9128 - val_loss: 0.1989 - val_accuracy: 0.9192\n",
      "Epoch 4/5\n",
      "1833/1833 [==============================] - 620s 338ms/step - loss: 0.2028 - accuracy: 0.9167 - val_loss: 0.1859 - val_accuracy: 0.9248\n",
      "Epoch 5/5\n",
      "1833/1833 [==============================] - 621s 339ms/step - loss: 0.1983 - accuracy: 0.9180 - val_loss: 0.1746 - val_accuracy: 0.9282\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2434002d880>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test with BiLSTM model\n",
    "model.fit(padded_train, y_train, batch_size=128, epochs=5, validation_data=(padded_test, y_test), callbacks=[early_stop])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "294e7d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test BILSTM\n",
    "preds = (model.predict(padded_test) > 0.1).astype(\"int32\")\n",
    "#second option for prediction based on the results o to 1\n",
    "#y_predict  = [1 if o>0.5 else 0 for o in model.predict(x_test_features)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "267bd83f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: spam_model_1\\spam_model_3\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: spam_model_1\\spam_model_3\\assets\n"
     ]
    }
   ],
   "source": [
    "#saving and loading the model.\n",
    "model.save(\"spam_model_1\\spam_model_3\")\n",
    "\n",
    "\n",
    "#load model\n",
    "s_model = tf.keras.models.load_model(\"spam_model_4\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1d564a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('spam_model_1\\my_model_3.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "54ebb4ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test- Results - Loss:0.174625426530838 - Accuracy:92.82114505767822%\n"
     ]
    }
   ],
   "source": [
    "preds2 = model.evaluate(padded_test, y_test, verbose= False)\n",
    "print(f'Test- Results - Loss:{preds2[0]} - Accuracy:{100*preds2[1]}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fc573017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 20, 24)            2453640   \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 480)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 500)               240500    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 200)               100200    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 100)               20100     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 2,814,541\n",
      "Trainable params: 2,814,541\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [1],\n",
       "       [1],\n",
       "       ...,\n",
       "       [1],\n",
       "       [1],\n",
       "       [1]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Defining prediction function option 1\n",
    "print(s_model.summary())\n",
    "#Change max_len according to the shape of the input/embedding layer\n",
    "def predict_spam(predict_msg):\n",
    "    with open(predict_msg, encoding=\"utf8\") as f:\n",
    "        test_content = [line.rstrip() for line in f]\n",
    "    test_arr = np.array(test_content)\n",
    "    t = Tokenizer()\n",
    "    max_len = 20\n",
    "    test_proc = t.texts_to_sequences((o)for o in test_arr)\n",
    "    test_proc = pad_sequences(test_proc, maxlen=max_len, padding='post')\n",
    "    #print(test_proc.shape)\n",
    "    pred = (s_model.predict(test_proc) > 0.1).astype(\"int32\")\n",
    "    return pred\n",
    "    index, j = np.where(pred == 0)\n",
    "    \n",
    "    print(index)\n",
    "    print(j)\n",
    "    print(len(index))\n",
    "    print(len(j))\n",
    "    selection = [test_arr[ind] for ind in index]\n",
    "    print(selection[:10])\n",
    "    \n",
    "predict_spam(\"C:/Users/steven/Documents/spam/Topic_02.en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62cb47ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
